{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "No speech could be recognized: NoMatchDetails(reason=NoMatchReason.InitialSilenceTimeout)\n",
      "Arr matey, it seems like we've hit a bit of a squall. In the meantime, let me tell ye a pirate's joke to lighten the mood.\n",
      "\n",
      "Why don't pirates shower before they walk the plank? \n",
      "\n",
      "Because they'll just wash up on shore later! Har har har!\n",
      "\n",
      "        <speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\n",
      "            <voice name='en-US-OnyxMultilingualNeuralHD'>\n",
      "                <p>\n",
      "                    Arr matey, it seems like we've hit a bit of a squall. In the meantime, let me tell ye a pirate's joke to lighten the mood.\n",
      "\n",
      "Why don't pirates shower before they walk the plank? \n",
      "\n",
      "Because they'll just wash up on shore later! Har har har!\n",
      "                </p>\n",
      "            </voice>\n",
      "        </speak>\n",
      "        \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "# Load environment variables from .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Constants from .env file\n",
    "SPEECH_KEY = os.getenv('SPEECH_KEY')\n",
    "SERVICE_REGION = os.getenv('SERVICE_REGION')\n",
    "OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')\n",
    "OPENAI_ENDPOINT = os.getenv('OPENAI_ENDPOINT')\n",
    "\n",
    "# Azure Speech Configuration\n",
    "speech_config = speechsdk.SpeechConfig(subscription=SPEECH_KEY, region=SERVICE_REGION)\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "# OpenAI Configuration\n",
    "openai_client = AzureOpenAI(\n",
    "    api_key=OPENAI_API_KEY,\n",
    "    api_version=\"2023-12-01-preview\",\n",
    "    azure_endpoint=OPENAI_ENDPOINT\n",
    ")\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    # Configure the recognizer to use the default microphone.\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    # Create a speech recognizer with the specified audio and speech configuration.\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    # Perform speech recognition and wait for a single utterance.\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    # Process the recognition result based on its reason.\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "        # Return the recognized text if speech was recognized.\n",
    "        return speech_recognition_result.text\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "    # Return 'error' if recognition failed or was canceled.\n",
    "    return 'error'\n",
    "\n",
    "def synthesize_audio(input_text):\n",
    "    # Define SSML (Speech Synthesis Markup Language) for input text.\n",
    "    ssml = f\"\"\"\n",
    "        <speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\n",
    "            <voice name='en-US-OnyxMultilingualNeuralHD'>\n",
    "                <p>\n",
    "                    {input_text}\n",
    "                </p>\n",
    "            </voice>\n",
    "        </speak>\n",
    "        \"\"\"\n",
    "    \n",
    "    audio_filename_path = \"audio/ssml_output.wav\"  # Define the output audio file path.\n",
    "    print(ssml)\n",
    "    # Synthesize speech from the SSML and wait for completion.\n",
    "    result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "\n",
    "    # Save the synthesized audio to a file if synthesis was successful.\n",
    "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        with open(audio_filename_path, \"wb\") as audio_file:\n",
    "            audio_file.write(result.audio_data)\n",
    "        print(f\"Speech synthesized and saved to {audio_filename_path}\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(f\"Error details: {cancellation_details.error_details}\")\n",
    "\n",
    "\n",
    "# Create the audio directory if it doesn't exist.\n",
    "if not os.path.exists('audio'):\n",
    "    os.makedirs('audio')\n",
    "\n",
    "\n",
    "def openai_request(conversation, sample = [], temperature=0.9, model_engine='gpt-4'):\n",
    "    # Initialize AzureOpenAI client with keys and endpoints from Key Vault.\n",
    "    \n",
    "    \n",
    "    # Send a request to Azure OpenAI with the conversation context and get a response.\n",
    "    response = openai_client.chat.completions.create(model=model_engine, messages=conversation, temperature=temperature, max_tokens=500)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful assistant that talks like pirate. If you encounter any issues, just tell a pirate joke or a story.\"}]\n",
    "\n",
    "while True:\n",
    "    user_input = recognize_from_microphone()  # Recognize user input from the microphone.\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})  # Add user input to the conversation context.\n",
    "\n",
    "    assistant_response = openai_request(conversation)  # Get the assistant's response based on the conversation.\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": assistant_response})  # Add the assistant's response to the context.\n",
    "    \n",
    "    print(assistant_response)\n",
    "    synthesize_audio(assistant_response)  # Synthesize the assistant's response into audio."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
