{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speak into your microphone.\n",
      "Recognized: Hello.\n",
      "Ahoy there, matey! How can this old sea dog be of service to ye today?\n",
      "\n",
      "        <speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\n",
      "            <voice name='en-US-OnyxMultilingualNeuralHD'>\n",
      "                <p>\n",
      "                    Ahoy there, matey! How can this old sea dog be of service to ye today?\n",
      "                </p>\n",
      "            </voice>\n",
      "        </speak>\n",
      "        \n",
      "Speech synthesized and saved to audio/ssml_output.wav\n",
      "Speak into your microphone.\n"
     ]
    }
   ],
   "source": [
    "import azure.cognitiveservices.speech as speechsdk\n",
    "from azure.identity import AzureCliCredential\n",
    "from azure.keyvault.secrets import SecretClient\n",
    "from openai import AzureOpenAI\n",
    "import os\n",
    "\n",
    "# Constants\n",
    "KEYVAULT_NAME = 'keyvaultmain713'  # Replace with your own Key Vault name\n",
    "\n",
    "# Azure Speech Configuration\n",
    "keyvault_client = SecretClient(f\"https://{KEYVAULT_NAME}.vault.azure.net/\", AzureCliCredential())\n",
    "speech_key = keyvault_client.get_secret('speech-api-key-sweden').value\n",
    "service_region = \"swedencentral\"\n",
    "speech_config = speechsdk.SpeechConfig(subscription=speech_key, region=service_region)\n",
    "speech_synthesis_voice_name = \"en-US-ShimmerMultilingualNeuralHD\"\n",
    "speech_config.speech_synthesis_voice_name = speech_synthesis_voice_name\n",
    "speech_synthesizer = speechsdk.SpeechSynthesizer(speech_config=speech_config)\n",
    "speech_config.speech_recognition_language=\"en-US\"\n",
    "\n",
    "\n",
    "def recognize_from_microphone():\n",
    "    audio_config = speechsdk.audio.AudioConfig(use_default_microphone=True)\n",
    "    speech_recognizer = speechsdk.SpeechRecognizer(speech_config=speech_config, audio_config=audio_config)\n",
    "\n",
    "    print(\"Speak into your microphone.\")\n",
    "    speech_recognition_result = speech_recognizer.recognize_once_async().get()\n",
    "\n",
    "    if speech_recognition_result.reason == speechsdk.ResultReason.RecognizedSpeech:\n",
    "        print(\"Recognized: {}\".format(speech_recognition_result.text))\n",
    "        #store the result to a txt file in transcripts folder named by timestamp\n",
    "\n",
    "        return speech_recognition_result.text\n",
    "\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.NoMatch:\n",
    "        print(\"No speech could be recognized: {}\".format(speech_recognition_result.no_match_details))\n",
    "    elif speech_recognition_result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = speech_recognition_result.cancellation_details\n",
    "        print(\"Speech Recognition canceled: {}\".format(cancellation_details.reason))\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(\"Error details: {}\".format(cancellation_details.error_details))\n",
    "            print(\"Did you set the speech resource key and region values?\")\n",
    "    return 'error'\n",
    "\n",
    "\n",
    "# Define your SSML\n",
    "def synthesize_audio(input_text):\n",
    "    \n",
    "    ssml = f\"\"\"\n",
    "        <speak version='1.0' xmlns='http://www.w3.org/2001/10/synthesis' xml:lang='en-US'>\n",
    "            <voice name='en-US-OnyxMultilingualNeuralHD'>\n",
    "                <p>\n",
    "                    {input_text}\n",
    "                </p>\n",
    "            </voice>\n",
    "        </speak>\n",
    "        \"\"\"\n",
    "    \n",
    "    audio_filename_path = \"audio/ssml_output.wav\"  # Define your audio file name\n",
    "    print(ssml)\n",
    "    result = speech_synthesizer.speak_ssml_async(ssml).get()\n",
    "\n",
    "    if result.reason == speechsdk.ResultReason.SynthesizingAudioCompleted:\n",
    "        # Save the audio to a file\n",
    "        with open(audio_filename_path, \"wb\") as audio_file:\n",
    "            audio_file.write(result.audio_data)\n",
    "        print(f\"Speech synthesized and saved to {audio_filename_path}\")\n",
    "    elif result.reason == speechsdk.ResultReason.Canceled:\n",
    "        cancellation_details = result.cancellation_details\n",
    "        print(f\"Speech synthesis canceled: {cancellation_details.reason}\")\n",
    "        if cancellation_details.reason == speechsdk.CancellationReason.Error:\n",
    "            print(f\"Error details: {cancellation_details.error_details}\")\n",
    "\n",
    "# Ensure the audio directory exists\n",
    "if not os.path.exists('audio'):\n",
    "    os.makedirs('audio')\n",
    "\n",
    "# keyvault authentication \n",
    "client = SecretClient(f\"https://{'keyvaultmain713'}.vault.azure.net/\", AzureCliCredential())\n",
    "\n",
    "# This is set to `azure`\n",
    "openai_client = AzureOpenAI(\n",
    "            api_key=client.get_secret('aoai-swissnorth-key').value,  \n",
    "            api_version=\"2023-12-01-preview\",\n",
    "            azure_endpoint = client.get_secret('aoai-swissnorth-endpoint').value\n",
    "        )\n",
    "\n",
    "def openai_request(conversation, sample = [], temperature=0.5, model_engine='gpt-4'):\n",
    "    response = openai_client.chat.completions.create(model=model_engine, messages=conversation, temperature=temperature, max_tokens=500)\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "conversation=[{\"role\": \"system\", \"content\": \"You are a helpful assistant that talks like pirate.\"}]\n",
    "\n",
    "while True:\n",
    "    user_input = recognize_from_microphone()\n",
    "    conversation.append({\"role\": \"user\", \"content\": user_input})\n",
    "\n",
    "    assistant_response = openai_request(conversation)\n",
    "\n",
    "    conversation.append({\"role\": \"assistant\", \"content\": assistant_response})\n",
    "    \n",
    "    print(assistant_response)\n",
    "    synthesize_audio(assistant_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
